# üì¶ New Modules & Features Summary

## ‚úÖ –ß—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ

### 1Ô∏è‚É£ –ú–æ–¥—É–ª—å —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö (`data_collectors.py`)

#### **WebScraperCollector** - –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤
```python
scraper = WebScraperCollector(llm_provider)

# –°–∫—Ä–∞–ø–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ URL
data = await scraper.scrape_urls(
    urls=["https://example.com/page1", "https://example.com/page2"],
    selector="article.content"
)

# –ò–ª–∏ –∫—Ä–∞—É–ª–∏–º –≤–µ—Å—å —Å–∞–π—Ç
data = await scraper.crawl_website(
    start_url="https://example.com/docs",
    max_pages=100,
    link_pattern="docs\\.example\\.com"
)

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
examples = scraper.convert_to_training_examples(data)
```

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (–±—ã—Å—Ç—Ä–æ!)
- ‚úÖ CSS —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ç–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏
- ‚úÖ –ö—Ä–∞—É–ª–∏–Ω–≥ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π —Å—Å—ã–ª–æ–∫
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Q&A —Å –ø–æ–º–æ—â—å—é LLM

---

#### **APICollector** - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å API
```python
collector = APICollector(api_key="YOUR_TOKEN")

# GitHub issues
issues = await collector.fetch_github_issues(
    repo="fastapi/fastapi",
    state="all",
    max_issues=200
)

# GitHub discussions
discussions = await collector.fetch_github_discussions(
    repo="microsoft/vscode",
    max_discussions=50
)

# StackOverflow
questions = await collector.fetch_stackoverflow_questions(
    tag="python",
    max_questions=100
)

# RSS feeds
entries = await collector.fetch_rss_feed(
    feed_url="https://blog.example.com/feed.xml",
    max_entries=50
)

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ –ø—Ä–∏–º–µ—Ä—ã
examples = collector.convert_to_training_examples(
    issues,
    data_type='github_issues',
    format_type='chat'
)
```

**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ API:**
- ‚úÖ GitHub Issues
- ‚úÖ GitHub Discussions (GraphQL)
- ‚úÖ StackOverflow Questions
- ‚úÖ RSS/Atom Feeds

---

#### **FileParserCollector** - –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–æ–≤
```python
parser = FileParserCollector(llm_provider)

# –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
text = parser.parse_text_file("docs/manual.txt")
data = parser.parse_json_file("data/knowledge.json")
markdown = parser.parse_markdown_file("docs/guide.md")

# –†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏
chunks = parser.chunk_text(text, chunk_size=1000, overlap=100)

# –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Q&A –ø–∞—Ä—ã
examples = parser.convert_documents_to_qa_pairs(
    documents=chunks,
    examples_per_doc=3
)
```

**–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:**
- ‚úÖ Plain Text (.txt)
- ‚úÖ JSON (.json)
- ‚úÖ JSONL (.jsonl)
- ‚úÖ Markdown (.md)

---

### 2Ô∏è‚É£ –ú–æ–¥—É–ª—å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏ (`deduplication.py`)

#### **DataDeduplicator** - –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
```python
deduplicator = DataDeduplicator(similarity_threshold=0.9)

# –ú–µ—Ç–æ–¥ 1: –¢–æ—á–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (hash)
unique, stats = deduplicator.deduplicate_dataset(
    examples=examples,
    format_type='chat',
    method='hash'
)

# –ú–µ—Ç–æ–¥ 2: –ù–µ—á–µ—Ç–∫–∏–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (fuzzy)
unique, stats = deduplicator.deduplicate_dataset(
    examples=examples,
    format_type='chat',
    method='fuzzy',
    similarity_threshold=0.88
)

print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['duplicates_removed']}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {stats['unique_count']}")
```

**–ú–µ—Ç–æ–¥—ã:**
- ‚úÖ `exact` - –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
- ‚úÖ `hash` - MD5/SHA256 —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ (–±—ã—Å—Ç—Ä–µ–µ)
- ‚úÖ `fuzzy` - –ù–µ—á–µ—Ç–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ (–ª—É—á—à–µ –¥–ª—è NLP)

---

#### **DataCleaner** - –û—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
```python
cleaner = DataCleaner()

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞
filtered, stats = cleaner.filter_dataset(
    examples=examples,
    format_type='chat',
    min_length=20,      # –ú–∏–Ω. –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    max_length=5000,    # –ú–∞–∫—Å. –¥–ª–∏–Ω–∞
    remove_toxic=True   # –£–¥–∞–ª–∏—Ç—å —Ç–æ–∫—Å–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
)

print(f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {stats['invalid_structure']}")
print(f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ: {stats['too_short']}")
print(f"–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ: {stats['too_long']}")
print(f"–¢–æ–∫—Å–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {stats['toxic_content']}")
print(f"–í–∞–ª–∏–¥–Ω—ã–µ: {stats['valid_count']}")
```

**–ü—Ä–æ–≤–µ—Ä–∫–∏:**
- ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (messages, instruction/output)
- ‚úÖ –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- ‚úÖ –¢–æ–∫—Å–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
- ‚úÖ –ö–æ–¥–∏—Ä–æ–≤–∫–∞
- ‚úÖ –õ–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã

---

#### **DataAnalyzer** - –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
```python
analyzer = DataAnalyzer()

# –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
analysis = analyzer.analyze_dataset(
    examples=examples,
    format_type='chat'
)

print(f"–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {analysis['total_examples']}")
print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª–∏–Ω—ã: {analysis['length_stats']}")
print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {analysis['metadata_analysis']}")
```

---

### 3Ô∏è‚É£ –°–∏—Å—Ç–µ–º–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π (`config_loader.py`)

#### –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–æ–≤ –∏–∑ YAML/JSON
```python
from config_loader import load_pipeline_config

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
config = load_pipeline_config("configs/my_pipeline.yaml")

print(f"–ò–º—è –ø–∞–π–ø–ª–∞–π–Ω–∞: {config.name}")
print(f"LLM Provider: {config.llm_provider.provider}")
print(f"–ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {len(config.data_sources)}")
```

#### –ü—Ä–∏–º–µ—Ä –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
```yaml
name: "my_pipeline"
description: "My custom pipeline"

llm_provider:
  provider: "openai"
  model: "gpt-5.1"
  api_key: "${OPENAI_API_KEY}"  # –ò–∑ env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö

data_sources:
  - type: "synthetic"
    config:
      domain: "support"
      count: 500

  - type: "web"
    config:
      urls: ["https://example.com"]

deduplication:
  enabled: true
  method: "fuzzy"

cleaning:
  enabled: true
  min_length: 20

output:
  dataset_name: "my_dataset"
  format: "jsonl"
```

**–§–∏—á–∏:**
- ‚úÖ YAML –∏ JSON —Ñ–æ—Ä–º–∞—Ç—ã
- ‚úÖ Env –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (`${VAR}`)
- ‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Å Pydantic
- ‚úÖ –®–∞–±–ª–æ–Ω—ã –∫–æ–Ω—Ñ–∏–≥–æ–≤

---

### 4Ô∏è‚É£ –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ (`versioning.py`)

#### **VersionManager** - –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏
```python
from versioning import version_manager

# –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–∏
version = version_manager.create_version(
    dataset_id=1,
    file_path="data/datasets/my_dataset.jsonl",
    description="–ò—Å—Ö–æ–¥–Ω–∞—è –≤–µ—Ä—Å–∏—è"
)

# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π
versions = version_manager.list_versions(dataset_id=1)

# –û—Ç–∫–∞—Ç –∫ –≤–µ—Ä—Å–∏–∏
version_manager.set_current_version(
    dataset_id=1,
    version_id="abc123"
)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π
diff = version_manager.compare_versions(
    dataset_id=1,
    version_id_1="abc123",
    version_id_2="def456"
)

print(f"–†–∞–∑–Ω–∏—Ü–∞ –ø—Ä–∏–º–µ—Ä–æ–≤: {diff['differences']['example_count_diff']}")
```

**–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
- ‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –û—Ç–∫–∞—Ç –∫ –ª—é–±–æ–π –≤–µ—Ä—Å–∏–∏
- ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π
- ‚úÖ –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
- ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–µ—Ä—Å–∏–∏

---

### 5Ô∏è‚É£ API Extensions (`api_extensions.py`)

#### –ù–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã (v2)

**–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö:**
- `POST /api/v2/collect/web-scrape` - –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤
- `POST /api/v2/collect/web-crawl` - –ö—Ä–∞—É–ª–∏–Ω–≥ —Å–∞–π—Ç–æ–≤
- `POST /api/v2/collect/github-issues` - GitHub issues
- `POST /api/v2/collect/stackoverflow` - StackOverflow

**–û–±—Ä–∞–±–æ—Ç–∫–∞:**
- `POST /api/v2/process/deduplicate` - –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
- `POST /api/v2/process/clean` - –û—á–∏—Å—Ç–∫–∞

**–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ:**
- `POST /api/v2/datasets/{id}/versions` - –°–æ–∑–¥–∞—Ç—å –≤–µ—Ä—Å–∏—é
- `GET /api/v2/datasets/{id}/versions` - –°–ø–∏—Å–æ–∫ –≤–µ—Ä—Å–∏–π
- `POST /api/v2/datasets/{id}/rollback` - –û—Ç–∫–∞—Ç

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
- `GET /api/v2/configs` - –°–ø–∏—Å–æ–∫ –∫–æ–Ω—Ñ–∏–≥–æ–≤
- `POST /api/v2/configs/execute` - –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω

---

## üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ (requirements.txt)

### –ù–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```txt
# Web Scraping & API Collection
aiohttp==3.9.3          # Async HTTP –∫–ª–∏–µ–Ω—Ç
beautifulsoup4==4.12.3  # HTML –ø–∞—Ä—Å–∏–Ω–≥
feedparser==6.0.11      # RSS/Atom feeds
lxml==5.1.0             # XML/HTML –æ–±—Ä–∞–±–æ—Ç–∫–∞

# Configuration
pyyaml==6.0.1           # YAML –∫–æ–Ω—Ñ–∏–≥–∏

# Progress
tqdm==4.66.2            # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä—ã
```

---

## üéØ –¢–∏–ø–∏—á–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ß–∞—Ç–±–æ—Ç —Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∏ IT-–∫–æ–º–ø–∞–Ω–∏–∏
```yaml
# configs/it_support.yaml
data_sources:
  # –°–∏–Ω—Ç–µ—Ç–∏–∫–∞ - –±–∞–∑–æ–≤—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏
  - type: "synthetic"
    config:
      domain: "support"
      subdomain: "technical"
      count: 500

  # –†–µ–∞–ª—å–Ω—ã–µ GitHub issues
  - type: "api"
    config:
      api_type: "github"
      params:
        repo: "your-company/product"
        state: "closed"
        max_issues: 300

  # FAQ —Å–æ —Å–∞–π—Ç–∞
  - type: "web"
    config:
      urls: ["https://yourcompany.com/faq"]
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è ‚Üí Q&A
```yaml
# configs/docs_qa.yaml
data_sources:
  - type: "web"
    config:
      start_url: "https://docs.yourproduct.com"
      max_pages: 200
      content_selector: "article.documentation"

llm_provider:
  provider: "openai"
  model: "gpt-5.1"  # –î–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ Q&A
```

### –°—Ü–µ–Ω–∞—Ä–∏–π 3: Code Assistant
```yaml
# configs/code_assistant.yaml
data_sources:
  # StackOverflow –≤–æ–ø—Ä–æ—Å—ã
  - type: "api"
    config:
      api_type: "stackoverflow"
      params:
        tag: "python"
        max_questions: 500

  # GitHub discussions
  - type: "api"
    config:
      api_type: "github"
      params:
        repo: "python/cpython"
```

---

## üöÄ –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
```bash
cd backend
pip install -r requirements.txt
```

### 2. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–æ–Ω—Ñ–∏–≥–æ–≤
```python
from config_loader import create_example_configs
create_example_configs()
```

### 3. –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ API
```bash
# –ó–∞–ø—É—Å–∫ FastAPI
uvicorn main:app --reload

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ
curl -X POST http://localhost:8000/api/v2/collect/web-scrape \
  -H "Content-Type: application/json" \
  -d @request.json
```

### 4. –ó–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ Python
```python
import asyncio
from data_collectors import WebScraperCollector
from llm_providers import create_provider

async def main():
    llm = create_provider("openai", model="gpt-5.1")
    scraper = WebScraperCollector(llm)

    data = await scraper.scrape_urls(
        urls=["https://example.com"],
        selector="article"
    )

    examples = scraper.convert_to_training_examples(data)
    return examples

examples = asyncio.run(main())
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
dataset-creator/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ data_collectors.py      # üÜï –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
‚îÇ   ‚îú‚îÄ‚îÄ deduplication.py        # üÜï –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ versioning.py           # üÜï –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py        # üÜï –ö–æ–Ω—Ñ–∏–≥–∏
‚îÇ   ‚îú‚îÄ‚îÄ api_extensions.py       # üÜï API v2
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt        # ‚úèÔ∏è –û–±–Ω–æ–≤–ª–µ–Ω
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ examples/               # üÜï –ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥–æ–≤
‚îÇ       ‚îú‚îÄ‚îÄ basic_synthetic.yaml
‚îÇ       ‚îú‚îÄ‚îÄ web_scraping.yaml
‚îÇ       ‚îú‚îÄ‚îÄ github_issues.yaml
‚îÇ       ‚îú‚îÄ‚îÄ stackoverflow.yaml
‚îÇ       ‚îî‚îÄ‚îÄ multi_source.yaml
‚îú‚îÄ‚îÄ UPGRADE_GUIDE.md            # üÜï –ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îî‚îÄ‚îÄ MODULES_SUMMARY.md          # üÜï –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

---

## üí° Best Practices

### 1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é
```python
# –ü–æ—Å–ª–µ –ª—é–±–æ–≥–æ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
unique, stats = deduplicator.deduplicate_dataset(
    examples=examples,
    method='fuzzy',
    similarity_threshold=0.88
)
```

### 2. –°–æ–∑–¥–∞–≤–∞–π –≤–µ—Ä—Å–∏–∏ –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
```python
# –ü–µ—Ä–µ–¥ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π, –æ—á–∏—Å—Ç–∫–æ–π, —É–ª—É—á—à–µ–Ω–∏–µ–º
version_manager.create_version(
    dataset_id=1,
    file_path=dataset_path,
    description="–ü–µ—Ä–µ–¥ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π"
)
```

### 3. –ò—Å–ø–æ–ª—å–∑—É–π –º—É–ª—å—Ç–∏—Å–æ—Ä—Å –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
```
40-60% - –°–∏–Ω—Ç–µ—Ç–∏–∫–∞ (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)
20-30% - Web scraping (—Ä–µ–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç)
20-30% - API –¥–∞–Ω–Ω—ã–µ (–∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å)
```

### 4. –í–∫–ª—é—á–∞–π quality control –¥–ª—è –≤–∞–∂–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
```yaml
quality_control:
  enabled: true
  threshold: 7.5
  auto_fix: true
```

---

## üéì –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. ‚úÖ –ò–∑—É—á–∏ –ø—Ä–∏–º–µ—Ä—ã –≤ `configs/examples/`
2. ‚úÖ –ó–∞–ø—É—Å—Ç–∏ —Ç–µ—Å—Ç–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω
3. ‚úÖ –°–æ–∑–¥–∞–π —Å–≤–æ–π –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è —Å–≤–æ–µ–π –∑–∞–¥–∞—á–∏
4. ‚úÖ –ó–∞–ø—É—Å—Ç–∏ —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
5. ‚úÖ –ü—Ä–∏–º–µ–Ω–∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—é + –æ—á–∏—Å—Ç–∫—É
6. ‚úÖ –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: quality control
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ

**–í—Å–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ—â–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤! üöÄ**
